{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concepts and terminology\n",
    "\n",
    "We start by introducing some basic concepts and terminology that are used frequently in the NLP domain. Make yourself familiar with these terms as we'll use them a lot in the following checkpoints as if we know them for quite some time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Corpus\n",
    "\n",
    "**Corpus (plural: corpora)** is the collection of texts we work on. So, if we're analyzing all the news that is published in The New York Times in 2018, then our corpus is the collection of all those news."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Document\n",
    "\n",
    "In NLP, the use of **document** slightly differs from that of spoken English and it might be confusing when you see the term *document* in NLP context. In NLP, we refer to the basic unit of observation as a **document**. Recall that in the previous modules, we worked with tabular data where each row represents an observation and columns represent the features. When working with text data, **we often refer to the observations as documents**. \n",
    "\n",
    "For example, say that we have a bunch of articles that are written by several authors and our task is to predict the author of a given article. In this case, *document* refers to the articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vocabulary\n",
    "\n",
    "**Vocabulary** refers to the list of all unique words in our corpus. Note that the vocabulary depends on the corpus at hand. So, for different corpora, there exists different vocabularies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stop words\n",
    "\n",
    "We call the words that are frequently encountered but hold relatively unimportant meanings as **stop words**. As we'll see in the later checkpoints, when we process our data, we remove all the stopwords from our dataset. Note that different languages have different stop words naturally and when we want to get rid of the stopwords from our dataset, we need to be careful about using the correct set of stopwords for that specific language. \n",
    "\n",
    "Although there's no a complete list of stopwords, we list some of them below for English:\n",
    "\n",
    "'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"\n",
    "\n",
    "That was a generalization when we said that we remove the stopwords from our data. In some NLP problems, it might make sense to retain them because stop words can make up part of meaningful phrases. For example, **\"master of the universe\" is more specific and informative than \"master\" and \"universe\" alone**. Yet, in many applications, we prefer to remove them before analyzing our text."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Token and tokenization\n",
    "\n",
    "In an NLP application, two of the most common concepts you'll come across is tokenization and token. We call each individual meaningful piece in a document as **token**, and the process of breaking up the document into these pieces is called **tokenization**. \n",
    "\n",
    "Tokens are generally words and punctuation. We may discard some tokens, such as punctuation, that we don't think add informational value. One class of potentially uninformative tokens are stop words as we touched upon before.\n",
    "\n",
    "Consider the following sentence:\n",
    "\n",
    "    I prefer to study natural language processing instead of watching Game of Thrones.\n",
    "    \n",
    "The tokens in this sentence are: \"I\", \"prefer\", \"to\", \"study\", \"natural\", \"language\", \"processing\", \"instead\", \"of\", watching\", \"game\", \"of\" and \"thrones\". If we want to keep punctuation in our work, then dot (.) is also a member of the tokens for the sentence above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Lemma\n",
    "\n",
    "Loosely speaking, we refer to the roots of the words as **lemma**. Yet, the following quote from Wikipedia gives a more concrete definition of it:\n",
    "\n",
    "> \"In morphology and lexicography, a lemma (plural lemmas or lemmata) is the canonical form, dictionary form, or citation form of a set of words (headword). In English, for example, run, runs, ran and running are forms of the same lexeme, with **run** as the lemma. Lexeme, in this context, refers to the set of all the forms that have the same meaning, and lemma refers to the particular form that is chosen by convention to represent the lexeme. In lexicography, this unit is usually also the citation form or headword by which it is indexed.\" - Wikipedia: https://en.wikipedia.org/wiki/Lemma_(morphology)\n",
    "\n",
    "The process of determining the lemmas of the tokens is called **lemmatization**. Now, let's determine the lemmas of the following sentence:\n",
    "\n",
    "    I prefer to study natural language processing instead of watching Game of Thrones.\n",
    "    \n",
    "| Token | Lemma |\n",
    "| --- | --- |\n",
    "| I | I |\n",
    "| prefer | prefer |\n",
    "| to | to |\n",
    "| study | study |\n",
    "| natural | natural |\n",
    "| language | language |\n",
    "| processing | processing |\n",
    "| instead | instead |\n",
    "| of | of |\n",
    "| watching | watch |\n",
    "| Game | game |\n",
    "| of | of |\n",
    "| Thrones | thrones |\n",
    "| . | . |\n",
    "\n",
    "You might be surprised that the lemmas of the all tokens except *watching* is the same as their original forms. Moreover, the lemma of the *watching* is *watch* but the lemma of the *processing* is *processing*. That's because the *ing* suffix makes the *processing* a noun where it makes *watching* an inflected form of *watch*. Hence, our lemmatization here takes that difference into consideration. That being said, **you should keep in mind that there are several ways of lemmatizing a token and hence you may encounter different lemmas for the same word depending on the lemmatizer you use**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Stem\n",
    "\n",
    "Another concept that's close to lemma is the **stem**. Stem refers to the core part of a word that never changes even in the different forms of that word. According to Wikipedia:\n",
    "\n",
    "> \"The stem is the part of the word that never changes even when morphologically inflected; a lemma is the base form of the word. For example, from \"produced\", the lemma is \"produce\", but the stem is \"produc-\". This is because there are words such as production. In linguistic analysis, the stem is defined more generally as the analyzed base form from which all inflected forms can be formed.\" - Wikipedia: https://en.wikipedia.org/wiki/Lemma_(morphology)\n",
    "\n",
    "Determining the stems of the tokens is known as **stemming**. We can think of stems as a more distilled form of the lemmas. When lemmatizing, we are aiming to retain the core meaning of the words that are shared among its different forms. In stemming, we also aim to do this by only retaining the same characters that convey the same meaning in all of the forms of a word.\n",
    "\n",
    "Now, let's derive the stems of the following sentence:\n",
    "\n",
    "    I prefer to study natural language processing instead of watching Game of Thrones.\n",
    "    \n",
    "| Token | Stem |\n",
    "| --- | --- |\n",
    "| I | I |\n",
    "| prefer | prefer |\n",
    "| to | to |\n",
    "| study | studi |\n",
    "| natural | natur |\n",
    "| language | languag |\n",
    "| processing | process |\n",
    "| instead | instead |\n",
    "| of | of |\n",
    "| watching | watch |\n",
    "| Game | game |\n",
    "| of | of |\n",
    "| Thrones | throne |\n",
    "| . | . |\n",
    "\n",
    "As you see, some stems seem strange and we may not be able to find them in an English dictionary. That is because, **a stemmer's job is to turn inflected forms of tokens into some common root without considering whether that root is a 'proper' word that can be found in a dictionary or not. Also note that, as in the case with lemmatizers, different stemmers exist and they may produce different stems for the same token**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Part of speech\n",
    "\n",
    "**Part of speech (POS)** refers to the particular class of words in a language. The words in the same class usually play a similar role in a sentence. According to Wikipedia:\n",
    "\n",
    "> \"In traditional grammar, a part of speech (abbreviated form: PoS or POS) is a category of words (or, more generally, of lexical items) which have similar grammatical properties. Words that are assigned to the same part of speech generally display similar behavior in terms of syntax—they play similar roles within the grammatical structure of sentences—and sometimes in terms of morphology, in that they undergo inflection for similar properties....Commonly listed English parts of speech are noun, verb, adjective, adverb, pronoun, preposition, conjunction, interjection, and sometimes numeral, article, or determiner.\" - [Wikipedia](https://en.wikipedia.org/wiki/Part_of_speech)\n",
    "\n",
    "Determining the **POS** of words is useful especially for distinguishing between homographs, words with the same spelling but different meaning (the umbrella term for this kind of linguistic feature is *polysemy*). For example, the word \"break\" is a noun in \"I need a break\" but a verb in \"I need to break the glass\".\n",
    "\n",
    "As an example, consider the following sentence:\n",
    "\n",
    "    I prefer to study natural language processing instead of watching Game of Thrones.\n",
    "    \n",
    "The part of speeches of the words of this sentence are as follows:\n",
    "\n",
    "| Token | POS tag |\n",
    "| --- | --- |\n",
    "| I | PRP |\n",
    "| prefer | VBP |\n",
    "| to | TO |\n",
    "| study | VB |\n",
    "| natural | JJ |\n",
    "| language | NN |\n",
    "| processing | VBG |\n",
    "| instead | RB |\n",
    "| of | IN |\n",
    "| watching | VBG |\n",
    "| Game | NNP |\n",
    "| of | IN |\n",
    "| Thrones | NNP |\n",
    "| . | . |\n",
    "\n",
    "Note that **there exists several POS taggers and each of them have its own terminology of POS tag names**. In the example above, PRP refers to the pronoun or personal, VBP refers to the present tense verb but not third-person singular etc. We'll not cover the details of each tag here, but the tools you'll use to derive the POS tags of the sentences have their documentation which explain these tags in detail.\n",
    "\n",
    "Lastly, **we want to bring to your attention that different POS taggers may produce different results for the same sentence**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Named entity\n",
    "\n",
    "**Named entities** are the words (or a group of words) in a sentence that indicate a predefined category of objects (entities). These categories include but not limited to person names, organizations, locations, time expressions, quantities, monetary values, percentages, etc. The task of determining the named entities in a sentence is known as **named entity recognition (NER)**.\n",
    "\n",
    "Consider the following sentence as an example:\n",
    "\n",
    "    The Federal Reserve decided to decrease the interest rates by 0.25% in their last FOMC meeting.\n",
    "    \n",
    "Here, *Federal Reserve* is the central bank of the US and hence it's an *organization* and *0.25%* is a *percentage*. The same caution that we made previously also applies here: **there are several named entity parsers and they may produce different results for the same sentence**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Dependency graph\n",
    "\n",
    "The **Dependency graph (also known as Dependency Tree)** of a sentence represents the grammatical structure and indicates the relationship between the words. These graphs often represented as a tree structure where the top word is called the **root** or **head**.  Dependency graph is useful in understanding how words relate to each other syntactically. The task of building the dependency graph is known as **dependency parsing**. \n",
    "\n",
    "Dependencies are a bit complicated – for a visual example of dependencies expressed as a tree, check the [About section of the Standford NLP Group Dependencies page](https://nlp.stanford.edu/software/stanford-dependencies.shtml). Stanford's NLP Group has had a lot of influence in this field, so you're likely to run across them frequently if you go deep into NLP. We aren't going to cover different methods of deriving the dependency graphs here but here we provide a visual representation of the dependency graph for this sentence:\n",
    "\n",
    "    I prefer to study natural language processing instead of watching Game of Thrones.\n",
    "    \n",
    "![dependency_graph.png](assets/dependency_graph.png)\n",
    "\n",
    "Last, we should mention that **several dependency parsers exist and each may result in different dependency graphs for the same sentence**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main NLP packages in the Python ecosystem\n",
    "\n",
    "There are several NLP packages in the Python ecosystem that you can use in your work. Even more, new packages are being added quite frequently due to the fast pace of advances in the NLP. However, in this module, we'll be using the most established ones. Specifically, we'll use:\n",
    "\n",
    "* **Scikit-learn**\n",
    "\n",
    "* **Natural Language Toolkit (NLTK)**\n",
    "\n",
    "* **SpaCy** and\n",
    "\n",
    "* **Gensim**\n",
    "\n",
    "We'll talk about the prominent features of these libraries and how to install them on your computer. But in case you want some other cool packages to play with, you can check:\n",
    "\n",
    "* **TextBlob**\n",
    "\n",
    "* **Stanford Core NLP** and\n",
    "\n",
    "* **Fasttext**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing when to use which package all depends on your use case. We recommend ***NLTK*** only as an education and research tool. Its modularized structure makes it excellent for learning and exploring NLP concepts, but it’s not meant for production. It’s the most famous Python NLP library, and it’s led to incredible breakthroughs in the field.  NLTK is also popular for education and research. NLTK claims to be an “an amazing library to play with natural language.” The major drawback of NLTK is that it’s heavy and slippery, and it has a steep learning curve. The second major weakness is that it’s slow and not production-ready.\n",
    "\n",
    "You can install NLTK by running the following command in your terminal (or command prompt if you're using Windows):\n",
    "\n",
    "```bash\n",
    "pip install nltk\n",
    "```\n",
    "\n",
    "***SpaCy*** is a new NLP library that’s designed to be fast, streamlined, and production-ready. It’s not as widely adopted, but if you’re building a new application, you should give it a try. SpaCy is minimal and opinionated, and it doesn’t flood you with options like NLTK does. Its philosophy is to only present one algorithm (the best one) for each purpose. You don’t have to make choices, and you can focus on being productive. Because it’s built on Cython, it’s also lightning-fast.\n",
    "\n",
    "You can install spaCy by running the following command in your terminal (or command prompt if you're using Windows):\n",
    "\n",
    "```bash\n",
    "pip install spacy\n",
    "```\n",
    "\n",
    "***Gensim*** is most commonly used for topic modeling and similarity detection. It’s not a general-purpose NLP library, but for the tasks it does handle, it does them well. Gensim is a well-optimized library for topic modeling and document similarity analysis. Among the Python NLP libraries listed here, it’s the most specialized. Its topic modeling algorithms, such as its Latent Dirichlet Allocation (LDA) implementation, are best-in-class. In addition, it’s robust, efficient, and scalable.\n",
    "\n",
    "You can install Gensim by running the following command in your terminal (or command prompt if you're using Windows):\n",
    "\n",
    "```bash\n",
    "pip install gensim\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
