{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you recall from the model preperation module of this bootcamp, a typical data science workflow/pipeline is as shown in the diagram below:\n",
    "\n",
    "![Data Science Workflow](assets/eda.png)\n",
    "\n",
    "When working with text, we need to modify the above process by taking into consideration the nature and hence the peculiarities of the text data. Look at the modified diagram below:\n",
    "\n",
    "![Data Preprocessing](assets/data_preprocessing.png)\n",
    "\n",
    "We see that the three steps in the exploratory data analysis process stay in their places. However, we modified the feature engineering step slightly. Let's go step by step to get into the data science workflow/pipeline when working with the text data:\n",
    "\n",
    "1. We need to clean our data to remove the problematic parts. In this checkpoint, we walk you through the basic steps of data cleaning for textual data.\n",
    "\n",
    "2. After we clean up our text, we need to do some data exploration to get to know our data better. In this module, we'll not dedicate a separate checkpoint for this step. This is mainly because the data exploration checkpoints of the model preparation module cover many techniques that we can also apply to the text data.\n",
    "\n",
    "3. The last step in data preprocessing is the feature engineering where we prepare the features that we'll use in the modeling phase. In this respect, text data is special and it requires different methods for creating features that will be used in the modeling. This step contains converting text data into numerical form which is also known as **language modeling**. We dedicate the next three checkpoints for this step.\n",
    "\n",
    "4. Note that the steps above are iterative and we may need to go back when we feel necessary.\n",
    "\n",
    "5. Once we prepared our numerical features, then we can jump into the modeling phase and apply machine learning techniques for different supervised and unsupervised tasks.\n",
    "\n",
    "In this checkpoint, we'll talk about some data cleaning methods for text data. Our focus will be on methods that are specific to text data. Hence, we skip the common methods that are also relevant to numerical data as those techniques are covered in the model preparation module. With this checkpoint, we also start introducing you the tools we'll be using throughout the module. Specifically, we'll make use of two NLP packages here: **NLTK** and **SpaCy**: \n",
    "\n",
    "* As we touched upon previously, NLTK is a seasoned package with great richness in its functionality. It is highly customizable and contains many models which are useful for learning NLP but are not state of the art anymore and not optimal for production code. \n",
    "\n",
    "* SpaCy is almost the direct opposite. It doesn't offer many models but rather it uses algorithms and methods that are considered state of the art at the moment. Note that, using only a limited number of algorithms and models generates a trade-off between flexibility and ease of use. SpaCy makes its choice by standing on the later side. This makes its usage very leaner. Furthermore, it's written in Cython (that is the Python code we write is translated into C) and hence it's considerably faster. Indeed, it's among the fastest NLP libraries available.\n",
    "\n",
    "In the rest of this module, we'll use the functionalities of NLTK and SpaCy interchangeably. Note that some functionalities are available in both of the packages but most of the time we'll use a functionality from a single package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The need for text preprocessing\n",
    "\n",
    "Most of the time, we'll be given a raw dataset consisting of many documents. But in order to work on them, we need to **organize the material in a convenient way** and **clean the noisy and problematic parts** that can harm our analysis, or at least increase the computation time without any informational gain.\n",
    "\n",
    "## Preparing the dataset\n",
    "\n",
    "By organizing the material we mean preparing a dataset that we used to work with. Specifically, we want a tabular dataset where rows represent the observations and columns represent the features. For example, if our dataset consists of articles that are written by several authors and our goal is to distinguish authors from their writing styles, then we want the articles and the associated authors to be represented in rows where the article and the author are stored in two columns. However, this was just an example of many sorts of other use cases. Let's give some other examples:\n",
    "\n",
    "* If our dataset comprises of books that consist of chapters and each chapter may be written by different authors, then we may need to represent chapters as rows.\n",
    "\n",
    "* If our aim is to summarize the articles, then we may need to derive the paragraphs from the articles and make each paragraph a row.\n",
    "\n",
    "* If we want to translate a language into another one, then we may need to prepare a dataset where two sentences from the two languages form a row.\n",
    "\n",
    "So, building up a dataset from a raw material depends on the goal we want to achieve with that dataset. \n",
    "\n",
    "In the following three checkpoints, we'll prepare our dataset in the manner highlighted above. For now, we'll clean our data.\n",
    "\n",
    "## Cleaning the dataset\n",
    "\n",
    "Before having a dataset in the proper form, the first step is to clean our dataset. When it comes to text data, **cleaning** means a lot of things and you shouldn't take it too narrowly. Some of the things when we talk about cleaning text data are the followings:\n",
    "\n",
    "* Correcting the typos and the misspelled words.\n",
    "\n",
    "* Dealing with the abbreviations.\n",
    "\n",
    "* Making all characters lowercase (or uppercase).\n",
    "\n",
    "* Removing the emojis if exist.\n",
    "\n",
    "* Removing the stopwords.\n",
    "\n",
    "* Normalizing the words (aka lemmatization or stemming).\n",
    "\n",
    "One can add several other items to the list. In this checkpoint, our focus will be on removing the stopwords and normalizing the text. However, here's a [good Kaggle kernel](https://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing) if you want to have an accommodating reading. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing in action\n",
    "\n",
    "Now that we've revised why we need to preprocess the text data, we can begin to work on a dataset using NLTK and SpaCy. First, if you haven't installed NLTK and SpaCy yet, you'll want to install the packages with:\n",
    "\n",
    "```bash \n",
    "pip install nltk\n",
    "```\n",
    "\n",
    "and \n",
    "\n",
    "```bash \n",
    "pip install spacy\n",
    "```\n",
    "\n",
    "And you also need to download the English model of SpaCy from your terminal (or command prompt) using:\n",
    "\n",
    "```bash\n",
    "python -m spacy download en\n",
    "```\n",
    "or more conveniently, you can do the same inside Jupyter notebook by running the following in a cell:\n",
    "\n",
    "```bash\n",
    "!python -m spacy download en\n",
    "```\n",
    "\n",
    "As usual, we begin with importing the libraries we'll use in this checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-275179478cde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we download the data we'll be using from NLTK's repository. Specifically, we'll use the **Gutenberg corpus** of the NLTK which is a sample from the [Gutenberg Project](https://www.gutenberg.org/). The project itself is a large collection of books written in English.\n",
    "\n",
    "Notice that installing NLTK isn't sufficient for these corpuses to be available for our use. We need to download them using the `nlt.download()` by providing the corpus name as parameter to it. In the next cell, we download this corpus.\n",
    "\n",
    "**Note**: There's also an interactive way of downloading data from NLTK repository. In this case, you need to run the following: \n",
    "```python\n",
    "nltk.download()\n",
    "```\n",
    "and it will launch an [interactive installer](http://www.nltk.org/data.html#interactive-installer). Using the installer, you can choose the \"corpora\" tab and download the Gutenberg corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the installer to download Gutenberg corpus\n",
    "nltk.download(\"gutenberg\")\n",
    "\n",
    "# Download the English models of SpaCy\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've acquired some data, let's dig in to look at it and get ready to clean things up. We're going to work specifically with two novels from the Gutenberg corpus: **Alice's Adventures in Wonderland** by Lewis Carroll, and **Persuasion** by Jane Austin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data we just downloaded\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# grab and process the raw data\n",
    "print(gutenberg.fileids())\n",
    "\n",
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "\n",
    "# print the first 100 characters of Alice\n",
    "print('\\nRaw:\\n', alice[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic text cleaning\n",
    "\n",
    "We first do some basic data cleaning in our data. Note that this kind of cleaning depends very much on the corpus we're working on. This is because all texts have their own peculiarities to be dealt with. As you'll see shortly, we'll remove some strings that are specific to the texts we're using.\n",
    "\n",
    "When modifying text data, using **regular expressions** is a common practice. We're also going to use *regular expressions* (specifically [re.sub()](https://docs.python.org/3/library/re.html#re.sub), short for \"substitute\") to identify and remove substrings we don't want. Specifically, we'll match those substrings with a regular expression and substitute in an empty string for them.\n",
    "\n",
    "We won't go into detail here about how regular expressions work, but you should be able to get a good sense for what's happening by reading the code. If you want more information the [Python Regular Expression HOWTO](https://docs.python.org/3/howto/regex.html) is an accessible starting point and reference, and [RegExr](http://regexr.com/) is a useful tool for visualizing and tinkering with regular expressions.\n",
    "\n",
    "We'll start our cleaning by removing the title. We'll match all text between square brackets and replace it with an empty string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this pattern matches all text between square brackets\n",
    "pattern = \"[\\[].*?[\\]]\"\n",
    "persuasion = re.sub(pattern, \"\", persuasion)\n",
    "alice = re.sub(pattern, \"\", alice)\n",
    "\n",
    "# print the first 100 characters of Alice again\n",
    "print(\"Title removed:\", alice[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll remove the chapter headings like `CHAPTER I`. Note that two novels have different styles of chapter headings. So, we deal with each novel one by one. This is quite usual in cleaning text data. As we said before, **all texts have their own peculiarities and cleaning them requires you to know those peculiarities**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# now we'll match and remove chapter headings\n",
    "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
    "alice = re.sub(r'CHAPTER .*', '', alice)\n",
    "\n",
    "# ok, what's it look like now?\n",
    "print('Chapter headings removed:', alice[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you were to read the two novels, you'd notice that there are a lot of \"new line\" characters and other types of extra whitespaces. So, we need to clean them up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove newlines and other extra whitespace by splitting and rejoining\n",
    "persuasion = ' '.join(persuasion.split())\n",
    "alice = ' '.join(alice.split())\n",
    "\n",
    "# all done with cleanup? let's see how it looks.\n",
    "print('Extra whitespace removed:', alice[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much of the things you saw as data cleaning so far were just a demonstration of what kind of problems you may encounter in a corpus. You can imagine a lot more than what we showed here. For example, if we were to work on a social media corpus, then we most likely would encounter with many emojis and abbreviations. So, dealing with them would also be a major problem in the data cleaning phase. Hence, **you should always be careful about what kind of corpus you have and what types of problems may occur in the text**.\n",
    "\n",
    "Since our text started to look okay, the next step is to tokenize our texts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "As you recall from the previous checkpoint, each individual meaningful piece from a text is called a **token**, and the process of breaking up the text into these pieces is called **tokenization**. **Tokenization is an important step in text preprocessing, because most of the time we generate the numerical representations of our texts from these tokens**. Hence, breaking up the text into tokens correctly is a crucial step for the success of the next steps of any data science workflow.\n",
    "\n",
    "Tokens are generally words and punctuation. In some NLP applications, you may see that people remove the punctuations from the text as if they are stopwords. There's no a single correct way of handling the punctuations and it's usually a matter of experimentation to determine the best way. In the following, we'll keep punctuations in our documents as we'll make use of them when separating our text into sentences. However, when we analyze our data, we check for them and don't include them in our analysis as you'll see shortly.\n",
    "\n",
    "Let's go ahead and use spaCy to parse our novels into tokens. When we call spaCy on the novel it will immediately and automatically parse it, tokenizing the string by breaking it into words and punctuation (and many other things we will explore):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "\n",
    "# all the processing work is done below, so it may take a while\n",
    "alice_doc = nlp(alice)\n",
    "persuasion_doc = nlp(persuasion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All our parsed documents are now stored in two variables we defined. SpaCy did a lot of good things when parsing the documents. Let's see what we have after the parsing happened:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's explore the objects we've built.\n",
    "print(\"The alice_doc object is a {} object.\".format(type(alice_doc)))\n",
    "print(\"It is {} tokens long\".format(len(alice_doc)))\n",
    "print(\"The first three tokens are '{}'\".format(alice_doc[:3]))\n",
    "print(\"The type of each token is {}\".format(type(alice_doc[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see from introspecting the spaCy objects above that we're playing around with [doc](https://spacy.io/docs/api/doc) and [token](https://spacy.io/docs/api/token) objects. These are the types that are defined by SpaCy.\n",
    "\n",
    "Now that we parsed our documents and get the tokens out of it, we can remove the stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing stopwords\n",
    "\n",
    "One of the important steps of text preprocessing is to remove the stopwords from the dataset. This is because they occur a lot in the text and most of the time they convey little meaning. So removing them benefits twice:\n",
    "\n",
    "1. We get rid off the noisy data.\n",
    "2. The size of the text diminishes and hence the computation time shortens.\n",
    "\n",
    "Removing stopwords with SpaCy is quite easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "alice_without_stopwords = [token for token in alice_doc if not token.is_stop]\n",
    "persuasion_without_stopwords = [token for token in persuasion_doc if not token.is_stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we just iterated over the tokens that are already made available by SpaCy during the parsing of the documents and exclude the token from the list if it's a stopword. \n",
    "\n",
    "Now, we store our tokens in two lists that are free of stopwords. Let's stop text processing a little bit and look at how frequent each token is in our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function to calculate how frequently words appear in the text\n",
    "def word_frequencies(text):\n",
    "    \n",
    "    # build a list of words\n",
    "    # strip out punctuation\n",
    "    words = []\n",
    "    for token in text:\n",
    "        if not token.is_punct:\n",
    "            words.append(token.text)\n",
    "            \n",
    "    # build and return a Counter object containing word counts\n",
    "    return Counter(words)\n",
    "\n",
    "# instantiate our list of most common words.\n",
    "alice_word_freq = word_frequencies(alice_without_stopwords).most_common(10)\n",
    "persuasion_word_freq = word_frequencies(persuasion_without_stopwords).most_common(10)\n",
    "print('\\nAlice:', alice_word_freq)\n",
    "print('Persuasion:', persuasion_word_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just take a moment and think about the 10 most common words in each novel. Do you see some differences that make sense to you?\n",
    "\n",
    "After the tokenization, the natural next step in text processing is lemmatization or stemming. We prefer to go with lemmatization here. But, you can also play with stemming if you like. Again, most of the time, determining which one of the lemmatization or stemming is the best choice is a matter of experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Lemmatization\n",
    "\n",
    "So far, we've tokenized our texts looked at whether certain words are present and how frequently they appear. We can process these words further to remove a little more noise from our data. Consider the words \"think\", \"thought\", and \"thinking\". They're related. They all share the same root word: the verb \"think\". Most of the times, we want to focus on the fact that the act of thinking comes up a lot in data, and not have that information split across all the different forms of \"think\".\n",
    "\n",
    "To focus in like this, we can reduce each word to its root that is to lemma and do our counts again. This time, we're building a count of *concepts* rather than just *words*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function to calculate how frequently lemas appear in the text\n",
    "def lemma_frequencies(text):\n",
    "    \n",
    "    # build a list of lemas\n",
    "    # strip out punctuation\n",
    "    lemmas = []\n",
    "    for token in text:\n",
    "        if not token.is_punct:\n",
    "            lemmas.append(token.lemma_)\n",
    "            \n",
    "    # build and return a Counter object containing lemma counts\n",
    "    return Counter(lemmas)\n",
    "\n",
    "# instantiate our list of most common lemmas\n",
    "alice_lemma_freq = lemma_frequencies(alice_without_stopwords).most_common(10)\n",
    "persuasion_lemma_freq = lemma_frequencies(persuasion_without_stopwords).most_common(10)\n",
    "print('\\nAlice:', alice_lemma_freq)\n",
    "print('Persuasion:', persuasion_lemma_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can realize, the top ten list changed. You can try to print more number of top lemmas and catch meaningful differences between the two novels.\n",
    "\n",
    "Alternatively, we can identify the lemmas common to one text but not the other. This may help us in understanding the differences between the two novels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_lemma_common = [pair[0] for pair in alice_lemma_freq]\n",
    "persuasion_lemma_common = [pair[0] for pair in persuasion_lemma_freq]\n",
    "print('Unique to Alice:', set(alice_lemma_common) - set(persuasion_lemma_common))\n",
    "print('Unique to Persuasion:', set(persuasion_lemma_common) - set(alice_lemma_common))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "These are examples of how you can do data exploration on text data. When it comes to text data, the limit is sky! So, use your imagination and find out more creative ways of analyzing the two novels based on the lemmas they have.\n",
    "\n",
    "We'll not go into the details but some syntactical properties can also help in this analysis. If you notice, the most frequent lemmas include person names. For the purpose of our analysis, we may need to eliminate them from the lists. In order to do this, we can derive the **named entities** in the texts and SpaCy has already derived named entities in the texts during parsing. If you like, you can go ahead and inspect the named entities.\n",
    "\n",
    "**Note**: We lemmatized our tokens to treat words with similar meanings as if they are the same. Apart from looking at lemmas, we could also perform a similar analysis by pulling out prefixes (`token.prefix_`) or suffixes (`token.suffix_`) from the tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentences\n",
    "\n",
    "Before closing this checkpoint, we want to mention about how to determine the sentences in a corpus. Beyond individual words, text can also be considered at the level of sentences. Using punctuation cues, we can split up text into sentences. Each sentence can then be summarized by, for example, using sentiment analysis to categorize sentences as having positive or negative sentiment. We may also be interested in how long sentences tend to be, and how many unique words make up a sentence. The sentence also provides *context* for the individual words, allowing us to draw even more information from each word.\n",
    "\n",
    "We get a lot of automatic sentence-level information from spaCy. The `doc.sents` property will give us each sentence as a [span](https://spacy.io/docs/api/span) object. Let's look at some of that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial exploration of sentences\n",
    "sentences = list(alice_doc.sents)\n",
    "print(\"Alice in Wonderland has {} sentences.\".format(len(sentences)))\n",
    "\n",
    "example_sentence = sentences[2]\n",
    "print(\"Here is an example: \\n{}\\n\".format(example_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at some metrics around this sentence\n",
    "example_words = [token for token in example_sentence if not token.is_punct]\n",
    "unique_words = set([token.text for token in example_words])\n",
    "\n",
    "print((\"There are {} words in this sentence, and {} of them are\"\n",
    "       \" unique.\").format(len(example_words), len(unique_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, sentence-level analysis can also be helpful in the data exploration phase.\n",
    "\n",
    "This is all about data cleaning and text preprocessing for now. It's your turn to complete the assignments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
