{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask's machine learning library, called **Dask-ml**, needs to be installed separately from the Dask. So, before moving forward, we need to install it from the command line as follows:\n",
    "\n",
    "```bash\n",
    "pip install dask-ml\n",
    "```\n",
    "\n",
    "Or you can run the same command inside jupyter notebook as follows:\n",
    "\n",
    "```python\n",
    "!pip install dask-ml\n",
    "```\n",
    "\n",
    "Let's start with how we can make use of the parallelization when training Scikit-learn models with Dask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelism in scikit-learn\n",
    "\n",
    "Scikit-learn library already offers a parallelization capability for some of its models. In particular, if you see `n_jobs` parameter in the documentation of a model, then it implies that you can set that parameter and scikit-learn dispatches the job to the cores of your computer. Scikit-learn does this using a library called [joblib](https://joblib.readthedocs.io/en/latest/).\n",
    "\n",
    "However, this parallelization in Scikit-learn is restricted to a single machine. Dask extends this and brings parallelization over a cluster. As we'll see in this checkpoint, if we use Dask as the backend to the joblib library, we can run Scikit-learn models over many machines in parallel.\n",
    "\n",
    "However as we said before, some models in Scikit-learn can't be trained on very large training data that doesn't fit into memory. For those situations, we'll talk about Dask's own machine learning module later in this checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Scikit-learn with Dask\n",
    "\n",
    "We'll start our discussion with how we can use Dask to enhance the parallelization capabilities of Scikit-learn. In doing this, we'll be using the [Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud) dataset from Kaggle as we did in the previous checkpoints. However, this time we'll build several machine learning models using Scikit-learn and Dask together. We'll see that using Dask, we can parallelize some of scikit-learn's machine learning models into several cores (or even machines on a cluster) when training. \n",
    "\n",
    "As we did in the previous checkpoints, we suggest starting a Dask client before running your code. By doing this, you'll be setting some configurations like the number of workers as well as you'll be able to monitor the execution of your codes.\n",
    "\n",
    "In our example, we set the number of workers to be four, number of threads per worker to be two and the memory limit for each worker to be 2GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:50743</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:50744/status' target='_blank'>http://127.0.0.1:50744/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>8</li>\n",
       "  <li><b>Memory: </b>8.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:50743' processes=4 threads=8, memory=8.00 GB>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from dask.distributed import Client, progress\n",
    "\n",
    "client = Client(n_workers=4, threads_per_worker=2, memory_limit='2GB')\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to load the dataset, we use Dask datarames. In the following cell, we load the dataset using the `.read_csv()` function of the Dask dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframes implement the Pandas API\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# This loads the data into Dask dataframe\n",
    "df = dd.read_csv('https://tf-assets-prod.s3.amazonaws.com/tf-curric/data-science/creditcard.csv', dtype={'Time': 'float64'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributing the training\n",
    "\n",
    "As we mentioned before, Scikit-learn can distribute the training of the models using [joblib](http://joblib.readthedocs.io/) library. However, this parallelization only supports a single machine. That is to say, we can only use the CPU cores of the machine that we run our models on. Dask can extend Scikit-learn's single machine parallelism to multiple machines. \n",
    "\n",
    "In demonstrating how we can parallelize the training of the Scikit-learn models using Dask, we train several classifiers on [Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud) dataset from Kaggle. Even more, we use four fold cross-validation to evaluate the performances of the models and then we do grid search cross-validation to tune the hyperparameters of the models. \n",
    "\n",
    "As stated in the official documentation of Dask, using Scikit-learn models with Dask support:\n",
    "\n",
    "> \"...is most useful for training large models on medium sized datasets. You may have a large model when searching over many hyper-parameters, or when using an ensemble method with many individual estimators. For too small datasets, training times will typically be small enough that cluster-wide parallelism isn't helpful. For too large datasets (larger than a single machine's memory), the Scikit-learn estimators may not be able to cope.\"\n",
    "\n",
    "We start with importing the libraries we'll use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import joblib\n",
    "from dask_ml.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we select the target variable and the features. We use the `Class` column as our target variable. It takes value 1 when the transaction is a fraud and 0 elsewhere. Our aim is to predict whether a given transaction is fraud or not. This problem is a classification problem and that is why we use classifiers.\n",
    "\n",
    "Note that the variables in this dataset except `Time`, `Class` and `Amount` are actually the principal components of some real variables that aren't exposed to us due to the confidentiality and privacy concerns. In the following models, we use the first three principal components as our feature set. This is for the sake of demonstration and speed. In the assignments, you'll be working with all of the variables.\n",
    "\n",
    "In the following cell, we also use a familiar function from Sickit-learn: `train_test_split()`. However we use it from Dask-ml's `model_selection` module. Similar to what Scikit-learn's method does, this method also randomly divide a dataset into train and test sets. However, Dask's `train_test_split()` can work in parallel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dask Series Structure:\n",
       "npartitions=3\n",
       "    int64\n",
       "      ...\n",
       "      ...\n",
       "      ...\n",
       "Name: Class, dtype: int64\n",
       "Dask Name: split, 3 tasks"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is our feature set\n",
    "X = df[[\"V1\", \"V2\", \"V3\", \"Amount\"]]\n",
    "\n",
    "# This is our target variable\n",
    "Y = df[\"Class\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "# Since our data can fit into memory\n",
    "# we persist them to the RAM.\n",
    "X_train.persist()\n",
    "X_test.persist()\n",
    "y_train.persist()\n",
    "y_test.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use Dask as the backend of the joblib library, we need to put our machine learning code inside the context manager of joblib. That is, we need to put the code we want to parallelize inside the following with statement:\n",
    "\n",
    "```python\n",
    "with joblib.parallel_backend('dask'):\n",
    "    Scikit-learn code here\n",
    "```\n",
    "\n",
    "For example, in the cell below, we train a random forest classifier using Dask as the parallelization library. In the following, we distribute the task across the cores of our computer. However, this code can be parallelized over multiple machines.\n",
    "\n",
    "Note that, we do four fold cross-validation in this code. So, we actually train four models and evaluate them on a different hold-out group. This alone is something that lays itself parallelization quite well. That being said, Dask can also parallelize the training of a single random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([7.6613791 , 6.18501663, 6.21940088, 7.33158302]),\n",
       " 'score_time': array([0.11823916, 0.13610721, 0.13948917, 0.12369299]),\n",
       " 'test_score': array([0.99766515, 0.99840247, 0.99859555, 0.99847267])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model = RandomForestClassifier()\n",
    "\n",
    "with joblib.parallel_backend('dask'):\n",
    "    scores = cross_validate(rf_model, X_train.compute(), y_train.compute(), cv=4)\n",
    "    \n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we move one step further and create a [`GridSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) object to tune the `max_depth` hyperparameter of the random forest classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest classifier\n",
    "rf_params = {\"max_depth\": [2, 4, 8, 16]}\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "\n",
    "grid_search_rf = GridSearchCV(rf_model,\n",
    "                           param_grid=rf_params,\n",
    "                           return_train_score=True,\n",
    "                           iid=True,\n",
    "                           cv=4,\n",
    "                           n_jobs=-1, \n",
    "                           scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can recall, when we do grid search with scikit-learn, we fit the model on the data as follows:\n",
    "\n",
    "```python\n",
    "grid_search.fit(X, y)\n",
    "```\n",
    "\n",
    "However, this time we want to distribute the training and cross validation over a cluster with Dask. In order to do this, we need to use the context manager provided by joblib library using the `with` statement of Python.\n",
    "\n",
    "Now, let's run the grid search over the hyperparameters of the random forest model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with joblib.parallel_backend('dask'):\n",
    "    grid_search_rf.fit(X_train.compute(), y_train.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained 16 different combinations of the model with four different values of the `max_depth` parameter and with four fold cross validation. Now, let's find out the best value for the `max_depth` parameter and get the AUC score on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best value is:  {'max_depth': 8}\n",
      "The test AUC score is:  0.9535513483491163\n"
     ]
    }
   ],
   "source": [
    "print(\"The best value is: \", grid_search_rf.best_params_)\n",
    "print(\"The test AUC score is: \", grid_search_rf.score(X_test.compute(), y_test.compute()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more on training scikit-learn models with distributed joblib, see the [dask-ml documentation](http://dask-ml.readthedocs.io/en/latest/joblib.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on Large Datasets\n",
    "\n",
    "So far, we only used our good old Scikit-learn library using Dask for the parallelization. However, most of the models in Scikit-learn are designed to work only on the data in the memory. Hence, training with large datasets that exceed the available memory isn't feasible with the approach we discussed so far.\n",
    "\n",
    "Here, we show that [Dask-ml](https://ml.dask.org/preprocessing.html#) library provides models that can be trained on very large datasets. All of the algorithms implemented in Dask-ml work well on larger than memory datasets. That being said, the model sets that are implemented in Dask-ml isn't very large. Hence, we can only find a restricted set of models in this library. To demonstrate the usage of Dask-ml, we'll use the same dataset above. But, you can try the following with a larger dataset that doesn't fit into the memory of your computer.\n",
    "\n",
    "The model we'll use is the logistic regression from Dask-ml's `linear_model` module. We start by importing the estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the logistic regression model and fit it to the training data. Note that when calling the `.fit()` function, we converted Dask dataframes X_train and y_train to Dask arrays by calling their `.values` attributes. **This is because, right now Dask-ml's glm based estimators just work with Dask arrays**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1.0, max_iter=100, multi_class='ovr',\n",
       "                   n_jobs=1, penalty='l2', random_state=None, solver='admm',\n",
       "                   solver_kwargs=None, tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train.values.compute(), y_train.values.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get the training and test scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score is:  0.8398388183067783\n",
      "Test score is:  0.8069194422938428\n"
     ]
    }
   ],
   "source": [
    "preds_train = lr.predict(X_train.values.compute())\n",
    "preds_test = lr.predict(X_test.values.compute())\n",
    "\n",
    "print(\"Training score is: \", roc_auc_score(preds_train, y_train.values.compute()))\n",
    "print(\"Test score is: \", roc_auc_score(preds_test, y_test.values.compute()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's all, we can close the client connection. For more information, you can read the official documentation of Dask-ml's linear regression [here](https://ml.dask.org/modules/generated/dask_ml.linear_model.LinearRegression.html#dask_ml.linear_model.LinearRegression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
