{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  What is a neuron?\n",
    "\n",
    "A neuron is the central unit of arithmetic in ANNs and it's also called **perceptron**. What a neuron does is to get some inputs and to spit out an output. The simplest way to think about it is as follows:\n",
    "\n",
    "**A neuron is similar to a linear regression model** where the output of the linear regression is fed into a non-linear function called **activation function**. As in the linear regression model, we have separate parameters for each feature and a bias. Here, the parameters are called the **weights**. However, don't get confused that neurons are not trained with OLS as we usually do when training the linear regression models. Instead of OLS, we use **backpropagation** and a variant of **Gradient Descent** algorithm to find out the correct values of the weights. We'll talk about activation functions, backpropagation and gradient descent in the later checkpoints. So, for now, just stay focused on what a neuron does.\n",
    "\n",
    "You may have already wondered as to why we call them **neurons**. That's because the neuron (or perceptron) concept is inspired by how human brains are working. In our brains, we have billions of neurons that are connected to each other. Hence, neurons constitute a huge network. The neurons we're talking about here are the analogical counterpart of the biological neurons in our brains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below sketches a single neuron. In a compact representation, a neuron gets inputs and apply a linear transformation to it. Then it feeds the output of this transformation to an activation function. The neuron spits out the result of the activation function. Notice that the output is a single numerical value (scalar) no matter how much features the neuron gets as input. It just aggregates them and apply a non-linear function and produces a single numerical value:\n",
    "\n",
    "![single neuron](assets/single_neuron.png)\n",
    "\n",
    "To formalize what we've talked about neurons so far, next we formulate a neuron mathematically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical representation of a neuron\n",
    "\n",
    "As we said before, inside a single neuron two mathematical operations are applied: \n",
    "\n",
    "1. Linear transformation of input: $y=b+x_1*w_1+x_2*w_2+...+x_n*w_n$\n",
    "\n",
    "2. Activation (output of the neuron): $Activation\\_fn(y) = Activation\\_fn(b+x_1*w_1+x_2*w_2+...+x_n*w_n)$\n",
    "\n",
    "where n represents the size of the input vector (or the number of features), $x_1,...,x_n$ represents the values in the input vector of size n and $b$ represents the **bias term**. $w_1,...,w_n$ represents the **weights** of the neuron. **$b$ and $w_i$'s are the parameters to be estimated**.\n",
    "\n",
    "Now that we know what a neuron is, let's look at what neurons constitute together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a layer?\n",
    "\n",
    "In its simplest form, we can define a layer as a collection of neurons that simultaneously process their inputs and simultaneously spit out their outputs. Any deep learning model comprises of layers and layers are stacked in a consecutive way. That is, the output of a layer will be the input of the consecutive layer.\n",
    "\n",
    "The diagram below sketches the basic architecture of an ANN:\n",
    "\n",
    "\n",
    "![ann architecture](assets/ann_architecture.png)\n",
    "\n",
    "Don't worry about the activation and the loss functions right now. We'll cover them later. But notice that in the figure above, the model has an input layer, two hidden layers, and an output layer. In ANNs as well as in any other type of deep learning models, there are broadly three groups of layers:\n",
    "\n",
    "* **Input layer** represents the input vector that is fed into the network. Typically this input vector has two dimensions (a,b):\n",
    "    * The first dimension (a) represents the number of observations.\n",
    "    * The second dimension (b) represents the number of features.\n",
    "    * However, some data may have three or more dimensions. For example, image data has 3 dimensions (a,b,c):\n",
    "        * The first dimension (a) represents the number of observations.\n",
    "        * The second dimension (b) represents the number of features.\n",
    "        * The third dimension (c) represents the color channel (RGB).\n",
    "* **Output layer** is the last layer in a deep model. It produces the output. The number of neurons in the output layer depends on the task at hand. Usually, the output layer consists of a single neuron if the task is regression. If the task is classification, then the number of neurons in the output layer is the same as the category size.\n",
    "* **Hidden layer** is the layer that lies between the input and the output layers. In order to have a deep model, we need to have at least one hidden layer. Typically in a deep model, we have several neurons in each hidden layer. There are many types of hidden layers:\n",
    "    * **Dense layer** is the one that is used in the ANNs. We'll be using this type of layers throughout this module.\n",
    "    * **Convolutional layer** is the layer that is used in the Convolutional Neural Networks (CNNs).\n",
    "    * **Recurrent layer** is the layer that is used in the Recurrent Neural Networks (RNNs). \n",
    "    * There are also other types of layers that are usually a hybrid of the above layers. But, talking about them is beyond the scope of this module.\n",
    "    \n",
    "As we said, our focus will be on the dense layer in this module. **A dense layer composes of several neurons and the number of neurons in a dense layer is a hyperparameter of the network that needs to be determined**. Next, let's look at how we formulate a dense layer mathematically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical representation of a dense layer\n",
    "\n",
    "For a dense layer where we have m number of neurons, the output of the layer can be represented as a set of m equations:\n",
    "\n",
    "$$Activation\\_fn(y_1)=b_1+x_{11}*w_{11}+x_{12}*w_{12}+...+x_{1n}*w_{1n}$$\n",
    "$$Activation\\_fn(y_2)=b_2+x_{21}*w_{21}+x_{22}*w_{22}+...+x_{2n}*w_{2n}$$\n",
    "$$.$$\n",
    "$$.$$\n",
    "$$.$$\n",
    "$$Activation\\_fn(y_m)=b_m+x_{m1}*w_{m1}+x_{m2}*w_{m2}+...+x_{mn}*w_{mn}$$\n",
    "\n",
    "In the matrix form:\n",
    "\n",
    "$$Activation\\_fn(Y)=Activation\\_fn(Wx+b)$$\n",
    "\n",
    "In $x_{ij}$, $b_i$ and $w_{ij}$, subscript i represents the index of the neurons and j represents the index of the input vector (the index of the features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many layers?\n",
    "\n",
    "A very natural question to ask when designing an ANN architecture is how much layers do we need to put in our model? First of all, you need to know that **the number of layers in a model is another hyperparameter of the model**. So, the number of layers need to be determined outside of the training process - say, by doing k-fold cross-validation. That being said, the reason behind having different layers is somehow complicated to understand.\n",
    "\n",
    "In theory, an ANN with a single hidden layer with a sufficiently large but finite number of neurons can represent any mathematical function. This is called [**universal approximation theorem**](https://en.wikipedia.org/wiki/Universal_approximation_theorem). But in practice, finding that ANN for a given function is something almost impossible. Yet, this theorem still gives a sense of how powerful tools deep models in terms of their representation capabilities.\n",
    "\n",
    "One of the reasons for having stacked layers in a network is this: In a shallow model, how do we get the interactions of the inputs? We can craft them by hand, but, for complex interactions, this is almost impossible. Consider images, videos or texts which are inherently complex data types. \n",
    "\n",
    "For example, how can we detect whether there's a human face in an image? Complex shapes like the shapes of ears, eyes, face are hard to come by hand-made designed interactions. But we can think of them as the combinations of the edges. Edges combine in some way and establish a shape like an ear, eye or face. In deep models, layers on top of layers enable features to interact with each other. **The interactions can become very complex and this is why deep learning models are good at representation learning**.\n",
    "\n",
    "When designing a deep learning model, you should always consider the following trade-off: \n",
    "\n",
    "* The deeper the model, the better it finds complex interactions and representation.\n",
    "    \n",
    "* But, increasing the layer also increases the number of parameters to be estimated and hence the computation time increases as well. \n",
    "    \n",
    "* Moreover, the more parameters to estimate, the more data we need to train our models.\n",
    "\n",
    "As a result, the number of layers and the number of neurons in each layer are hyperparameters to be determined. There's no single golden rule about these numbers and the best values for these numbers depend on the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this checkpoint, we touched the basic architecture of an ANN but didn't cover the activation functions, loss functions and how we train an ANN. We'll go over these in the remainder of this module. But before, we introduce TensorFlow and Keras libraries in the next checkpoint. We'll use them when building and training our deep learning models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
